from typing import List, Dict, Any, Tuple
import logging
from .base_pipeline import RagPipelineOllamaQdrant

logger = logging.getLogger(__name__)


class EnhancedRagPipeline:
    def __init__(self, qdrant_host: str, qdrant_port: int, collection_name: str):
        self._base_pipeline = RagPipelineOllamaQdrant(
            qdrant_host=qdrant_host,
            qdrant_port=qdrant_port,
            collection_name=collection_name,
        )
        self._collection_name = collection_name
        self._qdrant_host = qdrant_host
        self._qdrant_port = qdrant_port

        self._prompt = """
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.

–í–û–ü–†–û–°: 
{query}

–ö–û–ù–¢–ï–ö–°–¢ –ò–ó –î–û–ö–£–ú–ï–ù–¢–û–í:
{context}

–ò–ù–°–¢–†–£–ö–¶–ò–ò:
1. –í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –Ω–∞–π–¥–∏ –¢–û–ß–ù–û —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å
2. –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å –ø—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç - –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å –µ–≥–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏ —Ç–æ—á–Ω–æ
3. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —á–∞—Å—Ç–∏—á–Ω–∞—è - —É–∫–∞–∂–∏, —á—Ç–æ –∏–º–µ–Ω–Ω–æ –∏–∑–≤–µ—Å—Ç–Ω–æ, –∞ —á—Ç–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
4. –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—Ç–≤–µ—Ç–∞ - —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º
5. –ù–ï –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∏ –Ω–µ –¥–æ–º—ã—Å–ª–∏–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
6. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç –ª–æ–≥–∏—á–Ω–æ –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
7. –ü—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ –≤–æ–ø—Ä–æ—Å–∞ - –æ—Å–≤–µ—Ç–∏ –∫–∞–∂–¥—ã–π
8. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ—á–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

–û–¢–í–ï–¢ (–Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ):
"""

    def vectorize_query(self, *, text: str) -> List[float]:
        """–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞"""
        return self._base_pipeline.vectorize_query(text=text)

    def search_knn_by_query(self, *, query: str, top_k: int = 5) -> Any:
        """–ë–∞–∑–æ–≤—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        return self.search_knn_by_query_enhanced(
            query=query, top_k=top_k, use_hybrid=False
        )

    def search_knn_by_query_enhanced(
        self,
        query: str,
        top_k: int = 5,
        score_threshold: float = 0.4,
        use_hybrid: bool = True,
        fusion_alpha: float = 0.7,
        bm25_weight: float = 1.2,
        use_rerank: bool = True,
    ) -> List[Dict[str, Any]]:
        """
        –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ —Å BM25

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            score_threshold: –ü–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            use_hybrid: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
            fusion_alpha: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–ª–∏—è–Ω–∏—è –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            bm25_weight: –í–µ—Å –¥–ª—è BM25 —Å–∫–æ—Ä–∏–Ω–≥–∞
            use_rerank: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        try:
            logger.info(f" –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}' (–≥–∏–±—Ä–∏–¥–Ω—ã–π: {use_hybrid}, BM25)")

            if use_hybrid:
                search_result = self._base_pipeline.search_hybrid(
                    query=query,
                    top_k=min(top_k * 3, 30),  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–ª–∏—è–Ω–∏—è
                    score_threshold=score_threshold,
                    fusion_alpha=fusion_alpha,
                    bm25_weight=bm25_weight,
                    use_rerank=use_rerank,
                )
            else:
                search_result = self._base_pipeline.search_knn_by_query(
                    query=query,
                    top_k=min(top_k * 2, 20),
                    score_threshold=score_threshold,
                )

            results = [
                {
                    "id": hit.id,
                    "score": hit.score,
                    "payload": hit.payload,
                    "search_type": getattr(hit, "search_type", "semantic"),
                }
                for hit in search_result
                if hit.score >= score_threshold
            ]

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –±–µ—Ä–µ–º top_k
            results.sort(key=lambda x: x["score"], reverse=True)
            results = results[:top_k]

            # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–∏–ø–∞–º –ø–æ–∏—Å–∫–∞
            search_types = [r.get("search_type", "semantic") for r in results]
            type_counts = {}
            for st in search_types:
                type_counts[st] = type_counts.get(st, 0) + 1

            logger.info(
                f" –ù–∞–π–¥–µ–Ω–æ {len(results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –¢–∏–ø—ã –ø–æ–∏—Å–∫–∞: {type_counts}"
            )

            if results:
                scores_info = [f"{r['score']:.3f}" for r in results]
                logger.info(f" –°–∫–æ—Ä–∏–Ω–≥–∏: {scores_info}")

            return results

        except Exception as error:
            logger.error(f" –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ Qdrant: {error}")
            raise Exception(f"Error in searching in Qdrant's DB: {error}")

    def search_bm25_only(
        self, query: str, top_k: int = 5, bm25_weight: float = 1.2
    ) -> List[Dict[str, Any]]:
        """
        –¢–æ–ª—å–∫–æ BM25 –ø–æ–∏—Å–∫ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            bm25_weight: –í–µ—Å –¥–ª—è BM25 —Å–∫–æ—Ä–∏–Ω–≥–∞
        """
        try:
            logger.info(f" BM25 –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")

            bm25_results = self._base_pipeline.search_bm25(
                query=query, top_k=top_k, bm25_weight=bm25_weight
            )

            results = [
                {
                    "id": result["id"],
                    "score": result["score"],
                    "payload": result["payload"],
                    "search_type": "bm25",
                    "original_bm25_score": result.get("original_score", 0),
                }
                for result in bm25_results
            ]

            logger.info(f" BM25 –Ω–∞–π–¥–µ–Ω–æ {len(results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

            if results:
                bm25_scores = [f"{r['original_bm25_score']:.3f}" for r in results]
                logger.info(f"BM25 —Å–∫–æ—Ä–∏–Ω–≥–∏: {bm25_scores}")

            return results

        except Exception as error:
            logger.error(f" –û—à–∏–±–∫–∞ –ø—Ä–∏ BM25 –ø–æ–∏—Å–∫–µ: {error}")
            raise Exception(f"Error in BM25 search: {error}")

    def retrieve_context(self) -> str:
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        raise NotImplementedError("Use retrieve_context_enhanced instead")

    def retrieve_context_enhanced(
        self,
        query: str,
        top_k: int = 5,
        score_threshold: float = 0.4,
        temperature: float = 0.1,
        use_hybrid: bool = True,
        fusion_alpha: float = 0.7,
        bm25_weight: float = 1.2,
        use_rerank: bool = True,
    ) -> tuple[str, List[Dict[str, Any]]]:
        """
        –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º –ø–æ–∏—Å–∫–æ–º –∏ BM25

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            score_threshold: –ü–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            use_hybrid: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
            fusion_alpha: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–ª–∏—è–Ω–∏—è –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            bm25_weight: –í–µ—Å –¥–ª—è BM25 —Å–∫–æ—Ä–∏–Ω–≥–∞
            use_rerank: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        try:
            self._base_pipeline.temperature = temperature

            logger.info(
                f" –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: '{query}' "
                f"(–≥–∏–±—Ä–∏–¥–Ω—ã–π+BM25: {use_hybrid}, alpha: {fusion_alpha})"
            )

            search_results = self.search_knn_by_query_enhanced(
                query=query,
                top_k=top_k,
                score_threshold=score_threshold,
                use_hybrid=use_hybrid,
                fusion_alpha=fusion_alpha,
                bm25_weight=bm25_weight,
                use_rerank=use_rerank,
            )

            if not search_results:
                logger.warning(" –ü–æ –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                return (
                    "–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.",
                    [],
                )

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            highly_relevant = [r for r in search_results if r["score"] > 0.7]
            moderately_relevant = [
                r for r in search_results if 0.4 <= r["score"] <= 0.7
            ]
            low_relevant = [r for r in search_results if r["score"] < 0.4]

            logger.info(
                f" –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: –≤—ã—Å–æ–∫–∏—Ö={len(highly_relevant)}, "
                f"—É–º–µ—Ä–µ–Ω–Ω—ã—Ö={len(moderately_relevant)}, –Ω–∏–∑–∫–∏—Ö={len(low_relevant)}"
            )

            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —É—á–µ—Ç–æ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            context = self._format_context_by_relevance(
                highly_relevant, moderately_relevant
            )

            if not context:
                logger.warning("–ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—É—Å—Ç –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
                return (
                    "–ù–∞–π–¥–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å.",
                    [],
                )

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞
            self._log_search_effectiveness(search_results, query)

            formatted_prompt = self._prompt.format(query=query, context=context)

            logger.info(" –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM...")
            response = self._base_pipeline.invoke(formatted_prompt)

            logger.info(
                f" –û—Ç–≤–µ—Ç —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω, –¥–ª–∏–Ω–∞: {len(response.content)} —Å–∏–º–≤–æ–ª–æ–≤"
            )
            return response.content, search_results

        except Exception as error:
            logger.error(f" –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {error}")
            raise Exception(f"Error in retrieving context: {error}")

    def _log_search_effectiveness(
        self, search_results: List[Dict[str, Any]], query: str
    ) -> None:
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞"""
        if not search_results:
            return

        # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ –ø–æ–∏—Å–∫–∞
        search_types = [r.get("search_type", "semantic") for r in search_results]
        type_stats = {}
        for st in search_types:
            type_stats[st] = type_stats.get(st, 0) + 1

        # –°—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä
        avg_score = sum(r["score"] for r in search_results) / len(search_results)
        max_score = max(r["score"] for r in search_results)

        logger.info(f" –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞ –¥–ª—è '{query}':")
        logger.info(f"   –¢–∏–ø—ã –ø–æ–∏—Å–∫–∞: {type_stats}")
        logger.info(f"   –°—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä: {avg_score:.3f}")
        logger.info(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä: {max_score:.3f}")
        logger.info(f"   –í—Å–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(search_results)}")

    def _format_context_by_relevance(
        self, highly_relevant: List[Dict], moderately_relevant: List[Dict]
    ) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —Ç–∏–ø–∞ –ø–æ–∏—Å–∫–∞ –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
        context_parts = []

        if highly_relevant:
            context_parts.append("=== –í–´–°–û–ö–û–†–ï–õ–ï–í–ê–ù–¢–ù–´–ï –î–û–ö–£–ú–ï–ù–¢–´ ===")
            for i, result in enumerate(highly_relevant, 1):
                filename = result["payload"].get("file_name", "Unknown")
                chunk = result["payload"].get("chunk_text", "")
                score = result["score"]
                search_type = result.get("search_type", "semantic")

                # –û–±—Ä–µ–∑–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
                if len(chunk) > 1000:
                    chunk = chunk[:1000] + "... [–æ–±—Ä–µ–∑–∞–Ω–æ]"

                search_type_emoji = {
                    "semantic": "üîç",
                    "bm25": "üìù",
                    "hybrid": "üîÑ",
                }.get(search_type, "üìÑ")

                context_parts.append(
                    f"\n{search_type_emoji} –î–æ–∫—É–º–µ–Ω—Ç {i}: {filename} "
                    f"(—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.3f}, —Ç–∏–ø: {search_type})\n"
                    f"–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:\n{chunk}\n"
                    f"{'-'*50}"
                )

        if moderately_relevant:
            context_parts.append("\n=== –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –î–û–ö–£–ú–ï–ù–¢–´ ===")
            for i, result in enumerate(moderately_relevant, 1):
                filename = result["payload"].get("file_name", "Unknown")
                chunk = result["payload"].get("chunk_text", "")
                score = result["score"]
                search_type = result.get("search_type", "semantic")

                # –û–±—Ä–µ–∑–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
                if len(chunk) > 800:
                    chunk = chunk[:800] + "... [–æ–±—Ä–µ–∑–∞–Ω–æ]"

                search_type_emoji = {
                    "semantic": "üîç",
                    "bm25": "üìù",
                    "hybrid": "üîÑ",
                }.get(search_type, "üìÑ")

                context_parts.append(
                    f"\n{search_type_emoji} –î–æ–∫—É–º–µ–Ω—Ç {i}: {filename} "
                    f"(—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.3f}, —Ç–∏–ø: {search_type})\n"
                    f"–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:\n{chunk}\n"
                    f"{'-'*50}"
                )

        return "\n".join(context_parts) if context_parts else ""

    def compare_search_methods(
        self, query: str, top_k: int = 5, score_threshold: float = 0.4
    ) -> Dict[str, Any]:
        """
        –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        try:
            logger.info(f" –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞ –¥–ª—è: '{query}'")

            # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
            semantic_results = self.search_knn_by_query_enhanced(
                query=query, top_k=top_k, use_hybrid=False
            )

            # BM25 –ø–æ–∏—Å–∫
            bm25_results = self.search_bm25_only(query=query, top_k=top_k)

            # –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
            hybrid_results = self.search_knn_by_query_enhanced(
                query=query, top_k=top_k, use_hybrid=True
            )

            # –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π
            semantic_ids = {r["id"] for r in semantic_results}
            bm25_ids = {r["id"] for r in bm25_results}
            hybrid_ids = {r["id"] for r in hybrid_results}

            semantic_bm25_intersection = semantic_ids & bm25_ids
            all_intersection = semantic_ids & bm25_ids & hybrid_ids

            comparison_result = {
                "query": query,
                "semantic": {
                    "count": len(semantic_results),
                    "avg_score": self._calculate_avg_score(semantic_results),
                    "max_score": self._calculate_max_score(semantic_results),
                    "results": semantic_results,
                },
                "bm25": {
                    "count": len(bm25_results),
                    "avg_score": self._calculate_avg_score(bm25_results),
                    "max_score": self._calculate_max_score(bm25_results),
                    "results": bm25_results,
                },
                "hybrid": {
                    "count": len(hybrid_results),
                    "avg_score": self._calculate_avg_score(hybrid_results),
                    "max_score": self._calculate_max_score(hybrid_results),
                    "results": hybrid_results,
                },
                "analysis": {
                    "semantic_bm25_overlap": len(semantic_bm25_intersection),
                    "all_methods_overlap": len(all_intersection),
                    "unique_semantic": len(semantic_ids - bm25_ids),
                    "unique_bm25": len(bm25_ids - semantic_ids),
                },
            }

            logger.info(f" –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ:")
            logger.info(f"   –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö: {comparison_result['semantic']['count']}")
            logger.info(f"   BM25: {comparison_result['bm25']['count']}")
            logger.info(f"   –ì–∏–±—Ä–∏–¥–Ω—ã—Ö: {comparison_result['hybrid']['count']}")
            logger.info(
                f"   –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π+BM25: {comparison_result['analysis']['semantic_bm25_overlap']}"
            )

            return comparison_result

        except Exception as error:
            logger.error(f" –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞: {error}")
            return {"error": str(error)}

    def _calculate_avg_score(self, results: List[Dict[str, Any]]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ —Å–∫–æ—Ä–∞"""
        if not results:
            return 0.0
        return sum(r["score"] for r in results) / len(results)

    def _calculate_max_score(self, results: List[Dict[str, Any]]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Å–∫–æ—Ä–∞"""
        if not results:
            return 0.0
        return max(r["score"] for r in results)

    def clear_bm25_cache(self) -> Dict[str, Any]:
        """
        –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ BM25 (–ø–æ–ª–µ–∑–Ω–æ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏)

        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞
        """
        try:
            cache_keys = list(self._base_pipeline._bm25_cache.keys())
            documents_cache_keys = list(self._base_pipeline._documents_cache.keys())

            self._base_pipeline._bm25_cache.clear()
            self._base_pipeline._documents_cache.clear()

            result = {
                "cleared_bm25_cache": len(cache_keys),
                "cleared_documents_cache": len(documents_cache_keys),
                "remaining_memory": "–ö—ç—à BM25 –æ—á–∏—â–µ–Ω",
            }

            logger.info(
                f" –û—á–∏—â–µ–Ω –∫—ç—à BM25: {len(cache_keys)} –∏–Ω–¥–µ–∫—Å–æ–≤, "
                f"{len(documents_cache_keys)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
            )

            return result

        except Exception as error:
            logger.error(f" –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –∫—ç—à–∞ BM25: {error}")
            return {"error": str(error)}

    def get_search_stats(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–∏—Å–∫–∞

        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫—ç—à—É –∏ –º–µ—Ç–æ–¥–∞–º –ø–æ–∏—Å–∫–∞
        """
        try:
            bm25_cache_size = len(self._base_pipeline._bm25_cache)
            documents_cache_size = len(self._base_pipeline._documents_cache)

            stats = {
                "bm25_cache_size": bm25_cache_size,
                "documents_cache_size": documents_cache_size,
                "active_collections": list(self._base_pipeline._bm25_cache.keys()),
                "status": "active",
            }

            logger.info(
                f" –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–∏—Å–∫–∞: BM25 –∫—ç—à={bm25_cache_size}, "
                f"–¥–æ–∫—É–º–µ–Ω—Ç—ã={documents_cache_size}"
            )

            return stats

        except Exception as error:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {error}")
            return {"error": str(error)}
